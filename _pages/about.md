---
permalink: /
title: "Hello there, I am Zhipeng!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a scientist & engineer in the field of ML Systems. More specifically, I working on building LLM distributed training and Inference Systems for very large AI models (aka Large Language Models or Foundation Models). Unlike many other ML Systems researchers, I also work on modeling research, specifically focused on Efficient AI algorithms for LLM model compression & knowledge distillation, LLM post-training, Agentic Reinforcement Learning and Vision-Language Models (VLM). Some of the research findings are published in EMNLP, ICCV, ECCV, WACV, ICML and NeurIPS etc. 

I am Senior Manager/Senior Staff Research Scientist leading the AI research team at LinkedIn (subsidary of Microsoft). Previously I worked at AWS AI organization at Amazon, where I was leading the SageMaker Applied Science Team. I was also the Tech Lead Manager/Staff Software Engineer at Google[X]/Google Research, where I was building up the machine learning team for Moonshot project Chorus and engaged in projects of building coding agent using LLM (now part of Gemini). Before that I was Staff Research Scientist at Apple, where I lead the development of ML Algorithms for Sleep Tracking feature on Apple Watch. 

<!--# Selected Experiences-->

## Selected Recent Publications 

- **Scaling Down, Serving Fast: Compressing and Deploying Efficient LLMs for Recommendation Systems**  
  *Kayhan Behdin, Ata Fatahi, Qingquan Song, Yun Dai, Aman Gupta, Zhipeng Wang et.al* (2025). EMNLP.

## Open Source Contributions

I am the TSC Committer and code maintainer for DeepSpeed project, one of the most popular OSS libraries for LLM training. 