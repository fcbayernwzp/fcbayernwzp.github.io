---
permalink: /
title: "Hello there, I am Zhipeng!"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a seasoned scientist & engineer working in the field of Machine Learning Systems with a primary focus on the optimization of large-scale distributed training and inference systems for foundation models and large language models (LLMs). Distinct from many other ML Systems researchers, my work also extends into core modeling research. My core modeling research mainly focuses on efficient AIâ€”particularly LLM compression, knowledge distillation, LLM post-training methodologies, Agentic Reinforcement Learning and Vision-Language Models (VLM). Some of the research findings are published in *EMNLP, ICCV, ECCV, WACV, ICML and NeurIPS etc.* 

**Some of the representative ML Systems that I was involved in the development phase** including [DeepSpeed](https://github.com/deepspeedai/DeepSpeed), one of the most popular OSS LLM distributed training library (code maintainer and TSC Committer); [fmchisel](https://github.com/linkedin/fmchisel), the state-of-the-art Foundation Models Optimization research library (e.g. knowledge distillation,compression and quantization etc); and [Liger Kernel](https://github.com/linkedin/Liger-Kernel) (Kudos to Byron Hsu and Yun Dai et al for the awesome work, and I am happy to support the work as the manager). 

In the corporate world, I am Senior Manager/Senior Staff Research Scientist leading the Foundational AI algorithms organization at LinkedIn (subsidary of Microsoft). Previously I worked at AWS AI organization at Amazon, where I was leading the SageMaker Applied Science Team contributing to LLM Inference/Distributed Training and Evaluation Services. I was also the Tech Lead Manager/Staff Software Engineer at Google[X]/Google Research, where I was building up the machine learning team for the Moonshot project [Chorus](https://x.company/projects/chorus/) and engaged in AIDA project building [coding agent using LLM](https://x.company/projects/aida/) (now part of Gemini). I was also involved in PaLM model development work across Alphabet. Before that I was Staff Research Scientist at Apple, where I lead the development of ML Algorithms for [Sleep Apnea Detection on Apple Watch](https://www.apple.com/health/pdf/sleep-apnea/Sleep_Apnea_Notifications_on_Apple_Watch_September_2024.pdf). 

<!--# Selected Experiences-->
## Talks and Tutorials 
- **[Building Efficient Large-Scale Model Systems with DeepSpeed: From Open-Source Foundations to Emerging Research](https://supercomputing-system-ai-lab.github.io/events/asplos2026-llm-tutorial/index.html)**  
  Olatunji Ruwase, Minjia Zhang, Masahiro Tanaka, Zhipeng Wang, **ASPLOS 2026 Tutorial**

- **Ray x DeepSpeed Meetup: AI at Scale**  
  Olatunji Ruwase, Masahiro Tanaka, Vinay Sridhar and Zhipeng Wang, [slides](https://docs.google.com/presentation/d/1eM3mY6oW9GYkRy1Xz0iOnbbEr5T1t0JJXOM5BKtR-Ks/edit?slide=id.g38615d6b4c2_0_87#slide=id.g38615d6b4c2_0_87)  

## Selected Recent Publications 

- **[Scaling Down, Serving Fast: Compressing and Deploying Efficient LLMs for Recommendation Systems](https://aclanthology.org/2025.emnlp-industry.119.pdf)**  
  Kayhan Behdin, Ata Fatahi, Qingquan Song, Yun Dai, Aman Gupta, Zhipeng Wang et.al (2025). The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP), **oral presentation**

- **[EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)**
  Wenhui Zhu*, Xiwen Chen*, Zhipeng Wang*#, Shao Tang, Sayan Ghosh, Xuanzhao Dong, Rajat Koner, Yalin Wang (2026). IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). (* equal contribution, # corresponding author)

- **[Liger-Kernel: Efficient Triton Kernels for LLM Training](https://openreview.net/forum?id=36SjAIT42G)**
  Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen, Zhipeng Wang (2025). Championing Open-source DEvelopment in ML Workshop @ ICML25

- **[Local2Global query Alignment for Video Instance Segmentation](https://openaccess.thecvf.com/content/ICCV2025W/LSVOS/papers/Koner_Local2Global_query_Alignment_for_Video_Instance_Segmentation_ICCVW_2025_paper.pdf)** 
  Rajat Koner, Zhipeng Wang, Srinivas Parthasarathy, Chinghang Chen (2025). Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)

- **[Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction](https://arxiv.org/abs/2509.12464)** 
  Ryan Lucas, Kayhan Behdin, Zhipeng Wang, Qingquan Song, Shao Tang, Rahul Mazumder(2025). NeurIPS Efficient Reasoning Workshop. (in review with ICLR 2026) 

- **[Planner-R1: Reward Shaping Enables Efficient Agentic RL with Smaller LLMs](https://arxiv.org/abs/2509.25779)** 
  Siyu Zhu, Yanbin Jiang, Hejian Sang, Shao Tang, Qingquan Song, Biao He, Rohit Jain, Zhipeng Wang, Alborz Geramifard(2025). In review with ICLR 2026

- **[Scaling Up Efficient Small Language Models Serving and Deployment for Semantic Job Search](https://arxiv.org/pdf/2510.22101)** 
  Kayhan Behdin, Qingquan Song, Sriram Vasudevan, Jian Sheng, Xiaojing Ma, Z Zhou, Chuanrui Zhu, Guoyao Li, Chanh Nguyen, Sayan Ghosh, Hejian Sang, Ata Fatahi Baarzi, Sundara Raman Ramachandran, Xiaoqing Wang, Qing Lan, Qi Guo, Caleb Johnson, Zhipeng Wang*, Fedor Borisyuk. [preprint](https://arxiv.org/abs/2510.22101) (* Corresponding author)

## Open Source Contributions

I am the TSC Committer and code maintainer for [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) project, one of the most popular OSS libraries for LLM training. I also help maintain the [Liger Kernel](https://github.com/linkedin/Liger-Kernel) project, feel free to raise issues and contribute PRs. 
